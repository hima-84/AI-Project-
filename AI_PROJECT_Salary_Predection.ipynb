{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1faa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # %%\n",
    "import pandas as pd # display data frame (to et3aml with excel files)\n",
    "import numpy as np # edit in data frame and numeric operations\n",
    "import seaborn as sns # visualise the data like (pear plot(to know corilations))\n",
    "import matplotlib.pyplot as plt # like seaborn (box plot ,pie chart)\n",
    "from sklearn.model_selection import train_test_split,cross_val_score \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import cm #color maps\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "lol= pd.read_csv(r\"max.csv\") # read the excel files \n",
    "kn= pd.read_csv(r\"rfc.csv\") # read the excel files \n",
    "\n",
    "df= pd.read_csv(r\"Dataset.csv\") # read the excel files \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "import statistics\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# %%\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# %%\n",
    "df.info() # shows the data frame info \n",
    "\n",
    "# %%\n",
    "kn['salary']=kn['salary'].replace([1],' <=50K')\n",
    "kn['salary']=kn['salary'].replace([0],' >50K')\n",
    "\n",
    "\n",
    "kn.to_csv(\"rffcc.csv\")\n",
    "\n",
    "# %%\n",
    "kn\n",
    "\n",
    "# %%\n",
    "df\n",
    "\n",
    "# %%\n",
    "\n",
    "display(df.describe(include=\"object\")) #df.describe (describe numric datas only ) / include all describe all the data/ df.describe(include=\"objects \")describe all objects\n",
    "display(df.describe())\n",
    "\n",
    "# %% [markdown]\n",
    "# change data form range or discreat to 0 & 1\n",
    "# \n",
    "\n",
    "# %%\n",
    "df['salary']=df['salary'].replace([' <=50K'],'1')\n",
    "df['salary']=df['salary'].replace([' >50K'],'0')\n",
    "df['sex']=df['sex'].replace([' Female'],'1')\n",
    "df['sex']=df['sex'].replace([' Male'],'0')#replace data to 0 and 1\n",
    "\n",
    "# %% [markdown]\n",
    "# label enccoder to change objects to numbers to make it possiple to study the data\n",
    "\n",
    "# %%\n",
    "label_encoder=LabelEncoder()\n",
    "df['salary']=label_encoder.fit_transform(df['salary'])\n",
    "df['education']=label_encoder.fit_transform(df['education'])\n",
    "df['relationship']=label_encoder.fit_transform(df['relationship'])\n",
    "df['race']=label_encoder.fit_transform(df['race'])\n",
    "df['sex']=label_encoder.fit_transform(df['sex'])\n",
    "df['education']=label_encoder.fit_transform(df['education'])\n",
    "df['native-country']=label_encoder.fit_transform(df['native-country'])\n",
    "df['position']=label_encoder.fit_transform(df['position'])\n",
    "df['work-class']=label_encoder.fit_transform(df['work-class'])\n",
    "df['marital-status']=label_encoder.fit_transform(df['marital-status'])#change catigoral data to numiric\n",
    "\n",
    "# %% [markdown]\n",
    "# change the missing data to null \n",
    "# \n",
    "\n",
    "# %%\n",
    "df[\"work-class\"]=np.where(df[\"work-class\"]==\" ?\",np.nan,df[\"work-class\"])#[\"collumn name\"]=np.where(df[\"collomn name \"(if value of)]==\"what i wanna change\",change to,(else dont change) )\n",
    "df[\"education\"]=np.where(df[\"education\"]==\" ?\",np.nan,df[\"education\"])\n",
    "df[\"marital-status\"]=np.where(df[\"marital-status\"]==\" ?\",np.nan,df[\"marital-status\"])\n",
    "df[\"position\"]=np.where(df[\"position\"]==\" ?\",np.nan,df[\"position\"])\n",
    "df[\"relationship\"]=np.where(df[\"relationship\"]==\" ?\",np.nan,df[\"relationship\"])\n",
    "df[\"race\"]=np.where(df[\"race\"]==\" ?\",np.nan,df[\"race\"])\n",
    "df[\"sex\"]=np.where(df[\"sex\"]==\" ?\",np.nan,df[\"sex\"])\n",
    "df[\"native-country\"]=np.where(df[\"native-country\"]==\" ?\",np.nan,df[\"native-country\"])\n",
    "df[\"salary\"]=np.where(df[\"salary\"]==\" ?\",np.nan,df[\"salary\"])\n",
    "df['capital-gain']=np.where(df['capital-gain']==0,df['capital-gain'].mean(),df['capital-gain'])#changed all the 0 to mean \n",
    "df['capital-loss']=np.where(df['capital-loss']==0,df['capital-loss'].mean(),df['capital-loss'])\n",
    "\n",
    "# %%\n",
    "df.info()\n",
    "df.drop(['work-fnl'],axis=1)\n",
    "\n",
    "# %%\n",
    "df\n",
    "\n",
    "# %% [markdown]\n",
    "# corrlation\n",
    "\n",
    "# %%\n",
    "print(df.corr())\n",
    "#corrlation \n",
    "sns.heatmap(df.corr())\n",
    "#heat map of corrlation\n",
    "\n",
    "# %% [markdown]\n",
    "# removed coloms with law corr\n",
    "\n",
    "# %%\n",
    "df.drop(['native-country'],axis=1,inplace=True)\n",
    "df.drop(['race'],axis=1,inplace=True)\n",
    "df.drop(['marital-status'],axis=1,inplace=True)\n",
    "df.drop(['education'],axis=1,inplace=True)\n",
    "df.drop(['work-fnl'],axis=1,inplace=True)\n",
    "df.drop(['age'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#drop colomn with low corrliation\n",
    "df\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# removed the null rows\n",
    "\n",
    "# %%\n",
    "pew=df.dropna().reset_index()#remove all null rows.reset df as new one without na\n",
    "pew=pew.drop(labels=[\"index\"],axis=1)#remove collomn=['collomn name'],axis=1(cause collumn )0 if row \n",
    "\n",
    "\n",
    "# %%\n",
    "pew.shape\n",
    "#number of rows and colom\n",
    "pew\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# knn model begin\n",
    "# \n",
    "\n",
    "# %%\n",
    "Xk_train, Xk_test, Yk_train, Yk_test = train_test_split(pew.iloc[:,:8], pew.salary, random_state = 42,test_size=0.3)\n",
    "#split data to test and train[allrows,from colom 0 to 9 ],prdiction variable,shufle 42 times 30%test \n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "#2 catigories (0 and 1 )for salary\n",
    "knn.fit(Xk_train, Yk_train)\n",
    "Yk_pred = knn.predict(Xk_test)\n",
    "accuracy_score(Yk_test, Yk_pred)\n",
    "\n",
    "\n",
    "# %%\n",
    "ypred=knn.predict(Xk_train)\n",
    "accuracy_score(Yk_train, ypred)\n",
    "\n",
    "\n",
    "# %%\n",
    "grid_params = { 'n_neighbors' : [5,7,9,11,13,15],\n",
    "               'weights' : ['uniform','distance'],\n",
    "               'metric' : ['minkowski','euclidean','manhattan']}\n",
    "\n",
    "# %%\n",
    "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)\n",
    "g_res = gs.fit(Xk_train, Yk_train)\n",
    "\n",
    "\n",
    "# %%\n",
    "g_res.best_score_\n",
    "\n",
    "# %%\n",
    "g_res.best_params_\n",
    "\n",
    "# %%\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, weights = 'uniform',algorithm = 'brute',metric = 'minkowski')\n",
    "knn.fit(Xk_train, Yk_train)\n",
    "\n",
    "# %%\n",
    "# get a prediction\n",
    "y_hat = knn.predict(Xk_train)\n",
    "y_knn = knn.predict(Xk_test)\n",
    "\n",
    "# %%\n",
    "print('Training set accuracy: ', metrics.accuracy_score(Yk_train, y_hat))\n",
    "print('Test set accuracy: ',metrics.accuracy_score(Yk_test, y_knn))\n",
    "\n",
    "# %%\n",
    "\n",
    "Y_pred_KNN2 = knn.predict(lol)\n",
    "lol[\"salary\"]=Y_pred_KNN2\n",
    "lol.to_csv(\"knn.csv\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# knn end\n",
    "\n",
    "# %% [markdown]\n",
    "# standerization make all number on the same range to have equal importance\n",
    "# \n",
    "\n",
    "# %%\n",
    "#standerization make all number on the same range to have equal importance\n",
    "yrf=pew['salary']#LABEL\n",
    "xrf=pew.drop(columns='salary')#FEATURES\n",
    "def listOflists(lst):\n",
    "    return [[el] for el in lst]\n",
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "s=StandardScaler()\n",
    "pew['education-num']=flatten(s.fit_transform(listOflists(pew['education-num'].to_numpy().tolist())))\n",
    "pew['hours-per-week']=flatten(s.fit_transform(listOflists(pew['hours-per-week'].to_numpy().tolist())))\n",
    "pew['capital-gain']=flatten(s.fit_transform(listOflists(pew['capital-gain'].to_numpy().tolist())))\n",
    "pew['capital-loss']=flatten(s.fit_transform(listOflists(pew['capital-loss'].to_numpy().tolist())))\n",
    "\n",
    "sm = SMOTE(random_state=42)#to deal with unbalanced data\n",
    "x_res, y_res = sm.fit_resample(xrf,yrf)\n",
    "\n",
    "# %% [markdown]\n",
    "# start random forst\n",
    "\n",
    "# %%\n",
    "xrf_train,xrf_test,yrf_train,yrf_test=train_test_split(x_res,y_res,test_size=0.2,random_state=42,shuffle=True)#random state:to get the same result every time you run the code\n",
    "                            #CREATING AN INSTANCE OF RANDOM FOREST CLASSIFIER#\n",
    "rfc=RandomForestClassifier(random_state=42,class_weight='balanced',max_depth=25,n_estimators=300,max_features='sqrt')\n",
    "rfc.fit(x_res,y_res)\n",
    "\n",
    "\n",
    "# %%\n",
    "rfcpp=rfc.predict(lol)\n",
    "lol[\"salary\"]=rfcpp\n",
    "lol.to_csv(\"rfc.csv\")\n",
    "\n",
    "# %%\n",
    "                        #CROSS VALIDATION#\n",
    "\"\"\"cross_val_score(rfc, x_res, y_res, cv=5, scoring='accuracy')\n",
    "accuracy = np.array(cross_val_score(rfc, x_res, y_res, cv=5, scoring='accuracy'))\n",
    "print(accuracy)\n",
    "print(accuracy.mean())\"\"\"\n",
    "\n",
    "# %%\n",
    "                         #CHOOSING HYPERPARAMETERS#\n",
    "\"\"\"RandomForestClassifier().get_params()#display random forest hyper parameters\n",
    "n_estimators_list=list(range(200,350,50))\n",
    "max_depth_list=list(range(5,30,10))\n",
    "params_grid={'n_estimators':n_estimators_list,'max_depth':max_depth_list}\n",
    "num_combinations=1\n",
    "for k in params_grid.keys():num_combinations*=len(params_grid[k])\n",
    "print('num of combinations = ',num_combinations)\n",
    "print(params_grid)\n",
    "def my_roc_auc_score(model,x_res,y_res):return metrics.roc_auc_score(y_res,model.predict(x_res))\n",
    "rfc=GridSearchCV(estimator=RandomForestClassifier(class_weight='balanced'),param_grid=params_grid,cv=5,scoring=my_roc_auc_score,return_train_score=True,verbose=4)\n",
    "#rfc=RandomizedSearchCV(estimator=RandomForestClassifier(class_weight='balanced'),param_distributions=params_grid,cv=3,scoring=my_roc_auc_score,return_train_score=True,verbose=2)\n",
    "rfc.fit(x_res,y_res)\n",
    "\n",
    "print(rfc.best_params_)#display best combination of parameters\"\"\"\n",
    "\n",
    "# %% [markdown]\n",
    "# end random forst\n",
    "\n",
    "# %% [markdown]\n",
    "# start svm\n",
    "\n",
    "# %%\n",
    "training_set, test_set = train_test_split(pew, test_size = 0.3, random_state = 42)\n",
    "Xs_train = training_set.iloc[:,0:7].values\n",
    "Ys_train = training_set.iloc[:,8].values\n",
    "Xs_test = test_set.iloc[:,0:7].values\n",
    "Ys_test = test_set.iloc[:,8].values\n",
    "\n",
    "# %%\n",
    "classifier = SVC(kernel='rbf', random_state = 42)\n",
    "classifier.fit(Xs_train,Ys_train)\n",
    "Y_pred = classifier.predict(Xs_test)\n",
    "test_set[\"Predictions\"] = Y_pred\n",
    "cm = confusion_matrix(Ys_test,Y_pred)\n",
    "accuracy = float(cm.diagonal().sum())/len(Ys_test)\n",
    "accuracyt = float(cm.diagonal().sum())/len(Ys_train)\n",
    "print(\"\\nAccuracy Of SVM For The Given Dataset : \", accuracy)\n",
    "print(\"\\nAccuracy Of SVM For The Given Dataset tt: \", accuracyt)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# end svm\n",
    "\n",
    "# %% [markdown]\n",
    "# start logistics\n",
    "\n",
    "# %%\n",
    "# to compare our model's accuracy with sklearn model\n",
    "# Logistic Regression\n",
    "class LogitRegression() :\n",
    "\tdef __init__( self, learning_rate, iterations ) :\t\t\n",
    "\t\tself.learning_rate = learning_rate\t\t\n",
    "\t\tself.iterations = iterations\n",
    "\t\t\n",
    "\t# Function for model training\t\n",
    "\tdef fit( self, X, Y ) :\t\t\n",
    "\t\t# no_of_training_examples, no_of_features\t\t\n",
    "\t\tself.m, self.n = X.shape\t\t\n",
    "\t\t# weight initialization\t\t\n",
    "\t\tself.W = np.zeros( self.n )\t\t\n",
    "\t\tself.b = 0\t\t\n",
    "\t\tself.X = X\t\t\n",
    "\t\tself.Y = Y\n",
    "\t\t\n",
    "\t\t# gradient descent learning\n",
    "\t\t\t\t\n",
    "\t\tfor i in range( self.iterations ) :\t\t\t\n",
    "\t\t\tself.update_weights()\t\t\t\n",
    "\t\treturn self\n",
    "\t\n",
    "\t# Helper function to update weights in gradient descent\n",
    "\t\n",
    "\tdef update_weights( self ) :\t\t\n",
    "\t\tA = 1 / ( 1 + np.exp( - ( self.X.dot( self.W ) + self.b ) ) )\n",
    "\t\t\n",
    "\t\t# calculate gradients\t\t\n",
    "\t\ttmp = ( A - self.Y.T )\t\t\n",
    "\t\ttmp = np.reshape( tmp, self.m )\t\t\n",
    "\t\tdW = np.dot( self.X.T, tmp ) / self.m\t\t\n",
    "\t\tdb = np.sum( tmp ) / self.m\n",
    "\t\t\n",
    "\t\t# update weights\t\n",
    "\t\tself.W = self.W - self.learning_rate * dW\t\n",
    "\t\tself.b = self.b - self.learning_rate * db\n",
    "\t\t\n",
    "\t\treturn self\n",
    "\t\n",
    "\t# Hypothetical function h( x )\n",
    "\t\n",
    "\tdef predict( self, X ) :\t\n",
    "\t\tZ = 1 / ( 1 + np.exp( - ( X.dot( self.W ) + self.b ) ) )\t\t\n",
    "\t\tY = np.where( Z > 0.5, 1, 0 )\t\t\n",
    "\t\treturn Y\n",
    "\n",
    "\n",
    "# Driver code\n",
    "\n",
    "def main() :\n",
    "\t\n",
    "\t# Importing dataset\t\n",
    "\tdf = pd.read_csv( \"datasecondedition.csv\" )\n",
    "\tX = df.iloc[:,:-1].values\n",
    "\tY = df.iloc[:,-1:].values\n",
    "\t\n",
    "\t# Splitting dataset into train and test set\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(\n",
    "\tX, Y, test_size = 1/3, random_state = 0 )\n",
    "\t\n",
    "\t# Model training\t\n",
    "\tmodel = LogitRegression( learning_rate = 0.01, iterations = 1000 )\n",
    "\t\n",
    "\tmodel.fit( X_train, Y_train )\t\n",
    "\tmodel1 = LogisticRegression()\t\n",
    "\tmodel1.fit( X_train, Y_train)\n",
    "\t\n",
    "\t# Prediction on test set\n",
    "\tY_pred = model.predict( X_test )\t\n",
    "\tY_pred1 = model1.predict( X_test )\n",
    "\t\n",
    "\t# measure performance\t\n",
    "\tcorrectly_classified = 0\t\n",
    "\tcorrectly_classified1 = 0\n",
    "\t\n",
    "\t# counter\t\n",
    "\tcount = 0\t\n",
    "\tfor count in range( np.size( Y_pred ) ) :\n",
    "\t\t\n",
    "\t\tif Y_test[count] == Y_pred[count] :\t\t\t\n",
    "\t\t\tcorrectly_classified = correctly_classified + 1\n",
    "\t\t\n",
    "\t\tif Y_test[count] == Y_pred1[count] :\t\t\t\n",
    "\t\t\tcorrectly_classified1 = correctly_classified1 + 1\n",
    "\t\t\t\n",
    "\t\tcount = count + 1\n",
    "\t\t\n",
    "\tprint( \"Accuracy on test set by our model\t : \", (\n",
    "\tcorrectly_classified / count ) * 100 )\n",
    "\tprint( \"Accuracy on test set by sklearn model : \", (\n",
    "\tcorrectly_classified1 / count ) * 100 )\n",
    "\tif __name__ == \"__main__\" :\n",
    "    \t main()\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "#DecisionTreeClassifier\n",
    "Xd_train, Xd_test, Yd_train, Yd_test = train_test_split(pew.iloc[:,:8], pew.salary, random_state = 42,test_size=0.3)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt = dt.fit(Xd_train,Yd_train)\n",
    "y_pred = dt.predict(Xd_test)\n",
    "print(\"Usin DT:\")\n",
    "print(metrics.accuracy_score(Yd_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
